{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_reviews(chunk_size=50000):\n",
    "    \"\"\"\n",
    "    Process and save the data in smaller chunks to manage memory\n",
    "    \"\"\"\n",
    "    path = \"Electronics.jsonl.gz\"\n",
    "    chunk_counter = 0\n",
    "    chunk = []\n",
    "    \n",
    "    # First count total lines\n",
    "    print(\"Counting total lines...\")\n",
    "    with gzip.open(path, 'rt', encoding=\"utf8\") as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    \n",
    "    print(f\"Processing {total_lines:,} reviews in chunks of {chunk_size:,}\")\n",
    "    \n",
    "    # Process in chunks\n",
    "    with gzip.open(path, 'rt', encoding=\"utf8\") as f:\n",
    "        for i, line in tqdm(enumerate(f), total=total_lines):\n",
    "            d = json.loads(line.strip())\n",
    "            processed_review = {\n",
    "                'user_id': d['user_id'],\n",
    "                'rating': int(float(d['rating'])),\n",
    "                'helpful_vote': int(d['helpful_vote']),\n",
    "                'timestamp': int(d['timestamp']),\n",
    "                'asin': d['asin'],\n",
    "                'text': d['text'],\n",
    "                'title': d.get('title', ''),\n",
    "                'parent_asin': d.get('parent_asin', ''),\n",
    "                'verified_purchase': d['verified_purchase']\n",
    "            }\n",
    "            chunk.append(processed_review)\n",
    "            \n",
    "            # Save chunk when it reaches the chunk_size\n",
    "            if len(chunk) >= chunk_size:\n",
    "                df = pd.DataFrame(chunk)\n",
    "                df.to_parquet(f'chunks/chunk_{chunk_counter}.parquet')\n",
    "                chunk = []  # Clear chunk\n",
    "                chunk_counter += 1\n",
    "                \n",
    "    # Save any remaining reviews\n",
    "    if chunk:\n",
    "        df = pd.DataFrame(chunk)\n",
    "        df.to_parquet(f'chunk_{chunk_counter}.parquet')\n",
    "    \n",
    "    print(f\"Processing complete! Saved {chunk_counter + 1} chunks.\")\n",
    "\n",
    "# Run the processing\n",
    "process_and_save_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all chunk files from the chunks folder\n",
    "chunks = glob.glob('chunks/chunk_*.parquet')\n",
    "# Load and combine all chunks\n",
    "df = pd.concat([pd.read_parquet(f) for f in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and filtering chunks...\n",
      "Found 878 parquet files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 878/878 [02:52<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining filtered chunks...\n",
      "\n",
      "Sampling balanced dataset...\n",
      "\n",
      "Filtering Results:\n",
      "Total initial reviews: 43,886,944\n",
      "\n",
      "Reviews per rating after filtering:\n",
      "Rating 1: 4,577,077\n",
      "Rating 2: 1,968,106\n",
      "Rating 3: 2,498,497\n",
      "Rating 4: 4,589,194\n",
      "Rating 5: 22,433,872\n",
      "\n",
      "Final sampled dataset:\n",
      "rating\n",
      "1    87773\n",
      "2    87773\n",
      "3    87773\n",
      "4    87773\n",
      "5    87773\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def filter_chunks(percent=1):\n",
    "    \"\"\"\n",
    "    Load chunks, apply filters, and create balanced sample\n",
    "    \"\"\"\n",
    "    print(\"Loading and filtering chunks...\")\n",
    "    chunks = [f for f in sorted(os.listdir('chunks')) if f.endswith('.parquet')]\n",
    "    print(f\"Found {len(chunks)} parquet files\")\n",
    "    \n",
    "    # First pass: count and filter\n",
    "    rating_counts = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
    "    total_reviews = 0\n",
    "    filtered_chunks = []\n",
    "    \n",
    "    # Process all chunks\n",
    "    for chunk_file in tqdm(chunks, desc=\"Processing chunks\"):\n",
    "        df = pd.read_parquet(os.path.join('chunks', chunk_file))\n",
    "        total_reviews += len(df)\n",
    "        \n",
    "        df['date'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        filtered_df = df[\n",
    "            (df['date'].dt.year >= 2004) &  # Filter out reviews before 2004\n",
    "            (df['text'].str.len() >= 20) &   # Filter out reviews with less than 20 characters\n",
    "            (df['text'].str.len() <= 3520) &  # Filter out reviews with more than 3520 characters\n",
    "            (df['verified_purchase'] == True)  # Filter out unverified purchases\n",
    "        ]\n",
    "        \n",
    "        for rating in range(1, 6):\n",
    "            rating_counts[rating] += len(filtered_df[filtered_df['rating'] == rating])\n",
    "            \n",
    "        filtered_chunks.append(filtered_df)\n",
    "    \n",
    "    # Combine filtered chunks\n",
    "    print(\"Combining filtered chunks...\")\n",
    "    combined_df = pd.concat(filtered_chunks, ignore_index=True)\n",
    "    \n",
    "    # Sample 1% with equal distribution\n",
    "    print(\"\\nSampling balanced dataset...\")\n",
    "    target_total = int(total_reviews * (percent/100))\n",
    "    sample_size_per_rating = target_total // 5\n",
    "    \n",
    "    final_samples = []\n",
    "    for rating in range(1, 6):\n",
    "        rating_data = combined_df[combined_df['rating'] == rating]\n",
    "        if len(rating_data) >= sample_size_per_rating:\n",
    "            sampled = rating_data.sample(n=sample_size_per_rating, random_state=42)\n",
    "            final_samples.append(sampled)\n",
    "        else:\n",
    "            print(f\"Warning: Only {len(rating_data)} reviews available for rating {rating}\")\n",
    "            final_samples.append(rating_data)\n",
    "    \n",
    "    final_df = pd.concat(final_samples, ignore_index=True)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nFiltering Results:\")\n",
    "    print(f\"Total initial reviews: {total_reviews:,}\")\n",
    "    print(\"\\nReviews per rating after filtering:\")\n",
    "    for rating, count in rating_counts.items():\n",
    "        print(f\"Rating {rating}: {count:,}\")\n",
    "    \n",
    "    print(\"\\nFinal sampled dataset:\")\n",
    "    print(final_df['rating'].value_counts().sort_index())\n",
    "    \n",
    "    # Save processed dataset\n",
    "    final_df.to_parquet('processed_balanced.parquet')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Run filtering\n",
    "filtered_df = filter_chunks(percent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "# Load all chunks from the chunks folder\n",
    "print(\"Loading chunks...\")\n",
    "chunks = glob.glob('chunks/chunk_*.parquet')\n",
    "df = pd.concat([pd.read_parquet(f) for f in chunks], ignore_index=True)\n",
    "print(f\"Loaded {len(chunks)} chunks, total {len(df):,} reviews\")\n",
    "\n",
    "# The rest of your analysis code remains the same, just remove the DataFrame conversion\n",
    "# since we already have a DataFrame\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df['rating'].describe())\n",
    "\n",
    "# Rating distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['rating'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Average rating over time\n",
    "df['date'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "monthly_ratings = df.groupby(df['date'].dt.to_period('M'))['rating'].mean()\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_ratings.plot(kind='line')\n",
    "plt.title('Average Rating Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.show()\n",
    "\n",
    "# Helpful votes analysis\n",
    "print(\"\\nHelpful Votes Statistics:\")\n",
    "print(df['helpful_vote'].describe())\n",
    "\n",
    "# Verified vs unverified purchase ratings\n",
    "print(\"\\nAverage Rating by Verified Purchase:\")\n",
    "print(df.groupby('verified_purchase')['rating'].mean())\n",
    "\n",
    "# User Analysis\n",
    "user_review_counts = df['user_id'].value_counts()\n",
    "user_avg_ratings = df.groupby('user_id')['rating'].mean()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(user_review_counts, bins=50)\n",
    "plt.title('Distribution of Reviews per User')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Count')\n",
    "print(\"\\nUser Review Statistics:\")\n",
    "print(user_review_counts.describe())\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(user_avg_ratings, bins=50)\n",
    "plt.title('Distribution of Average User Ratings')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Review Text Analysis\n",
    "df['text_length'] = df['text'].str.len()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['text_length'], bins=50)\n",
    "plt.title('Distribution of Review Lengths')\n",
    "plt.xlabel('Review Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='rating', y='text_length', data=df)\n",
    "plt.title('Review Length by Rating')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nText Length Statistics:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "# Correlation between text length and helpful votes\n",
    "correlation = df['text_length'].corr(df['helpful_vote'])\n",
    "print(f\"\\nCorrelation between review length and helpful votes: {correlation:.3f}\")\n",
    "\n",
    "# Product Analysis\n",
    "product_review_counts = df['asin'].value_counts()\n",
    "product_avg_ratings = df.groupby('asin')['rating'].mean()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(product_review_counts, bins=50)\n",
    "plt.title('Distribution of Reviews per Product')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(product_avg_ratings, bins=50)\n",
    "plt.title('Distribution of Average Product Ratings')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nProduct Statistics:\")\n",
    "print(f\"Total unique products: {df['asin'].nunique()}\")\n",
    "print(\"\\nReviews per product:\")\n",
    "print(product_review_counts.describe())\n",
    "\n",
    "# Time analysis\n",
    "df['year'] = df['date'].dt.year\n",
    "yearly_counts = df.groupby('year').size()\n",
    "yearly_ratings = df.groupby('year')['rating'].mean()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "yearly_counts.plot(kind='bar')\n",
    "plt.title('Number of Reviews by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Reviews')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "yearly_ratings.plot(kind='bar')\n",
    "plt.title('Average Rating by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verified Purchase Analysis\n",
    "verified_stats = df.groupby('verified_purchase').agg({\n",
    "    'rating': ['count', 'mean', 'std'],\n",
    "    'helpful_vote': 'mean',\n",
    "    'text_length': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nVerified Purchase Analysis:\")\n",
    "print(verified_stats)\n",
    "\n",
    "# Overall Summary Statistics\n",
    "summary_stats = {\n",
    "    'Total Reviews': len(df),\n",
    "    'Unique Users': df['user_id'].nunique(),\n",
    "    'Unique Products': df['asin'].nunique(),\n",
    "    'Average Rating': df['rating'].mean(),\n",
    "    'Median Rating': df['rating'].median(),\n",
    "    'Average Review Length': df['text_length'].mean(),\n",
    "    'Verified Purchase %': (df['verified_purchase'] == 'Y').mean() * 100,\n",
    "    'Average Helpful Votes': df['helpful_vote'].mean()\n",
    "}\n",
    "\n",
    "print(\"\\nOverall Summary Statistics:\")\n",
    "for key, value in summary_stats.items():\n",
    "    print(f\"{key}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
